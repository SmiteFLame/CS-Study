# 빅데이터

## Hadoop

분산 환경에서 빅데이터를 저장하고 처리할 수 있는 자바 기반의 오픈소스 프레임 워크

- 하둡 소프트웨어 라이브러리는 간단한 프로그래밍 모델을 사용하여 여러 대의 컴퓨터 클러스터에 대규모 데이터 세트를 분산 처리할 수 있게 해주는 프레임 워크이다.

### 구성요소

1. 하둡 분산형 파일 시스템(HDFS)

- 하둡 네트워크 연결된 기기에 데이터를 저장하는 분산 시스템
- HDFS는 여러 기계에 대용량 파일을 나눠서 저장한다.
- 데이터를 여러 서버에 중복해서 저장함으로써 데이터 안정성을 얻는다.

  1. HDFS는 데이터를 저장하면, 다수의 노드에 복제 데이터도 함께 저장해서 데이터 유실을 막는다.
  2. HDFS에 파일을 저장하거나, 저장된 파일을 조회하려면 스트리밍 방식으로 데이터에 접근해야 한다.
  3. 한번 저장된 데이터를 수정할 수 없고 읽기만 가능해서 데이터 무결성을 유지한다.
  4. 데이터 수정은 불가능 하지만 파일 이동, 삭제, 복사할 수 있는 인터페이스르 제공한다.

  - Master/slave 구조를 통해서 하나의 네임 노드와 파일 시스템을 관리하고 클라이언트의 접근을 통제하는 마스터 서버로 구성된다.

2. 맵 리듀스(MapReduce)

- 대용량의 처리를 위한 분산 프로그래밍 모델, 소프트웨어 프레임워크
- 맵 리듀스 프레임워크를 이용하면 대규모 분산 컴퓨터 환경에서 대량의 데이터를 병렬로 분석 가능
- 프로그래머가 직접 작성하는 맵과 리듀스라는 두 개의 메소드로 구성

1. 흩어져 있는 데이터를 수직화
2. 그 데이터를 각각의 종류 별로 모은다(Map)
3. 필터링과 Sorting을 거쳐 데이터를 뽑아낸다(Reduce)

### 장점

1. 오픈 소스 라이선스에 대한 비용 부담이 적음
2. 시스템을 중단하지 않고, 장비의 추가가 용이(Scale Out)
3. 일부 장비에 장애가 발생하더라도 전체 시스템 사용성에 영향이 적음
4. 저렴한 구축 비용과 비용 대비 빠른 데이터 처리

### 단점

1. HDFS에 저장된 데이터 변경 불가
2. 실시간 데이터 분석 같이 처리해야 하는 작업에는 부적합
3. 너무 많은 버전과 부실한 서포트
4. 설정의 어려움

## HBase

Hadoop의 HDFS위에 만들어진 분산 컬럼 기반의 데이터 베이스

- 구조화된 대용량 데이터에 빠른 임의 접근을 제공하는 구글의 빅 테이블과 비슷한 데이터 모델을 가지며, HDFS의 데이터에 대한 실시간 임의 읽기/쓰기 기능을 제공
- 사용자는 HBase나 HDFS에 직접 데이터를 저장할 수 있고, 사용자는 데이터를 읽고 접근하는 것은 HBase를 통한 임의 접근을 이용한다.

- 고정 컬럼 스키마가 없다.
- 수평적으로 확장성이 있어 큰 테이블에 적합하다.

- Transaction이 존재하지 않는다.
- RDBMS에 비해서 덜 구조화된 데이터에 적합하다

## Spark(Apache Spark)

인 메모리 기반의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임 워크

- DISK I/O 기반으로 동작하는 Hadoop과 다르게 인메모리를 사용하여 속도가 최소 1000배는 빠르다.

- 데이터 실시간 스트리밍 처리 니즈를 충족한다.

- 하둡 + 스파크: 하둡의 Yarn 위에 Spark를 얹고, 실시간성이 필요한 데이터는 스파크로 처리하는 방식

### Spark Application

실제 일을 수행하는 역할을 담당한다

1. Driver: 한 개의 노드에서 실행되며, 스파크 전체의 main 함수를 실행한다.

- 어플리케이션 내 정보의 유지 관리, 익스 큐터의 실행 및 실행 분석, 배포 등의 역할을 수행한다.
- 사용자가 구성한 사용자 프로

2. Executor: 다수의 worker 노드에서 실행되는 프로세스

- Driver가 할당한 작업을 수행하여 결과를 반환한다.

- 1개의 스카프 어플리케이션은 1개의 Driver와 N개의 Executor가 존재한다.
- Executor는 Cluster Manager에 의하여 해당 스파크 어플리케이션에 할당된다.

- 해당 스파크 어플리케이션이 완전히 종료된 후 할당에서 해방된다
- 다른 스파크 어플리케이션 간의 직접적인 데이터 공유는 불가능하다.

### Cluster Manager

스파크 어플리케이션 사이에 자원을 중계해주는 역할을 담당한다.

- 스파크 어플리케이션의 리소스를 효율적으로 분배하는 역할을 담당한다.
- Executor에 Task를 할당하고 관리하기 위해서 클러스터 매니저에 의존한다.

- 사용 가능한 클러스터 매니저: Spark StandAlone, Hadoop Yarn, Mesos, Kubernetes등

### 실행 과정

1. 사용자가 Spark-submit을 통해 어플리케이션을 제출한다.
2. Spark Driver가 main을 실행하며, Spark Context를 생성한다.
3. Spark Context가 Cluster Manager와 연결된다.
4. Spark Driver가 Cluster Manager로 부터 Executor 실행을 위한 리소스를 요청한다.
5. Spark Context는 작업 내용을 task 단위로 분할하여 Excutor에 보낸다
6. 각 Executor는 작업을 수행하고, 결과를 저장한다.

- 즉, 사용자 프로그램을 수행하기 위해서, Spark Driver 내의 Spark Context가 Job을 task 단위로 쪼갠다.
  - Cluster Maanger로 부터 할당받은 Executor로 task를 넘긴다.

### 구성요소

1. Spark Core: 메인 컴포넌트로 작업 스케쥴링, 메모리 관리, 장애 복구와 같은 기본적인 기능을 제공하고, RDD, Dateset, DataFrame을 이용한 스파크 연산을 처리한다.
2. Spark Library: 빅데이터 처리를 위한 라이브러리
   - park SQL: 이를 이용하여 작업을 생성하고 처리한다.
   - park Streaming: 실시간 데이터 스트림을 처리하는 컴포넌트이다.
   - MLib: 스파크 기반의 머신러닝 기능을 제공하는 컴포넌트이다.
   - GraphX: 분산형 그래프 프로세싱이 가능하게 해주는 컴포넌트이다.
3. Cluster Manager: 스파크 작업을 운영하는 클러스터 관리지이다

### 장점

1. 인메모리 기반의 데이터 처리로 빠르다

- 인메모리: 데이터스토리지의 메인 메모리에 설치되어 운영되는 방식의 데이터베이스 방식 관리 시스템
  - 즉, 하드 디스크가 아닌 메인 메모리에 올려서 서비스를 수행 하는 것을 말한다.

2. 어플리케이션 형태의 빅데이터 통합환경을 제공
3. 실시간 데이터 프로세싱 지원
4. sparkml이라는 머신러닝 패키지 지원
5. Scala기반으로 작성되어 있음, 어플리케이션 코드로는 JAVA, Python 지원
6. Hadoop의 데이터 저장소와 아주 쉽게 연동된다.

### 단점

1. 자체 파일 시스템이 존재하지 않는다: 데이터를 가져오거나 저장할 때 Hadoop을 연동하여 사용해야 한다.
2. 비싸다: 데이터가 클 경우 병목현상이 일어난다.
3. 빈약학 ML 알고리즘
4. 데이터 처리가 유저에게 편리하지 않다.
5. 학습 비용이 크다.

## Kafka

LinkedIn에서 개발한 분산 메시징 시스템으로 대용량 실시간 로그처리에 특화된 아키텍처 설계를 통하여 기존 메시징 시스템보다 우수한 TPS를 보여준다.

- 분산 스트리밍 플랫폼
- 데이터 파이프 라인 구성시, 주로 사용되는 오픈소스 솔루션
- 대용량의 실시간 로그처리에 특화되어 있는 솔류션
- 데이터 유실없이 안적하게 전달하는 것이 주목적인 메시지 시스템
- 클러스티링이 가능하므로, Fault-Tolerant한 안정적인 아키텍처와 빠른 퍼포먼스로 데이터처리
- 수평적으로 서버의 Scale-Out이 가능함
- pub-sub 모델의 메시지 큐

### 구성 요소

- Event: Consumer가 데이터를 주고 받는 단위
- Producer: 이벤트를 게시(Post)하는 클라이언트 어플리케이션을 의미한다.
- Consumer: 이러한 Topic을 구독하고 이로 부터 얻어낸 Event를 처리하는 클라이언트 어플리케이션
- Topic: Event가 쓰이는 곳, Producer는 이 Topic에 Event를 게시한다.
  - Consumer는 Topic으로 부터 이벤트를 가져와 ㅓ리한다.

### 빅데이터 관련 내용 정리

- Hadoop: 대용량의 데이터를 적은 비용으로 더 빠르게 분석할 수 있는 플랫폼
  - 여러 대의 컴퓨터로 데이터를 분석하고 저장하는 방식(HDFS)
- HBase: HDFS위에 만들어진 분산 컬럼 기반의 데이터베이스
- Kafka: 실시간으로 기록 스트림을 게시, 구독, 저장 및 처리할 수 있는 분산 데이터 스트리밍 플랫폼
- Spark: 빅데이터 처리를 위한 오픈소스 분산 처리 플랫폼
  - Spark는 MapReduce에 대한 Hadoop 강화 기능이다.
  - Spark는 데이터를 처리하고 메모리에 보관하면서 속도가 최대 100배 정도 빠르다
